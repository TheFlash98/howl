<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Howl: Howl</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Howl
   &#160;<span id="projectnumber">1.0</span>
   </div>
   <div id="projectbrief">Wake word detection modeling toolkit for Firefox Voice, supporting open datasets like Speech Commands and Common Voice.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Howl </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_README"></a> <a href="https://pypi.org/project/howl/"><img src="https://img.shields.io/pypi/v/howl?color=brightgreen" alt="PyPI" class="inline"/></a> <a href="https://opensource.org/licenses/MPL-2.0"><img src="https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg" alt="License: MPL 2.0" style="pointer-events: none;" class="inline"/></a></p>
<p>Wake word detection modeling for Firefox Voice, supporting open datasets like Google Speech Commands and Mozilla Common Voice.</p>
<p>Citation:</p>
<div class="fragment"><div class="line">@inproceedings{tang-etal-2020-howl,</div>
<div class="line">    title = &quot;Howl: A Deployed, Open-Source Wake Word Detection System&quot;,</div>
<div class="line">    author = &quot;Tang, Raphael and Lee, Jaejun and Razi, Afsaneh and Cambre, Julia and Bicking, Ian and Kaye, Jofish and Lin, Jimmy&quot;,</div>
<div class="line">    booktitle = &quot;Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)&quot;,</div>
<div class="line">    month = nov,</div>
<div class="line">    year = &quot;2020&quot;,</div>
<div class="line">    publisher = &quot;Association for Computational Linguistics&quot;,</div>
<div class="line">    url = &quot;https://www.aclweb.org/anthology/2020.nlposs-1.9&quot;,</div>
<div class="line">    doi = &quot;10.18653/v1/2020.nlposs-1.9&quot;,</div>
<div class="line">    pages = &quot;61--65&quot;</div>
<div class="line">}</div>
</div><!-- fragment --><h1>Quickstart Guide</h1>
<ol type="1">
<li>Install PyAudio and <a href="https://pytorch.org">PyTorch 1.5+</a> through your distribution's package system.</li>
<li>Install Howl using <code>pip</code></li>
</ol>
<div class="fragment"><div class="line">pip install howl</div>
</div><!-- fragment --><ol type="1">
<li>To immediately use a pre-trained Howl model for inference, we provide the <code>client</code> API. The following example (also found under <code><a class="el" href="hey__fire__fox_8py.html">examples/hey_fire_fox.py</a></code>) loads the "hey_fire_fox" pretrained model with a simple callback and starts the inference client.</li>
</ol>
<div class="fragment"><div class="line">from howl.client import HowlClient</div>
<div class="line"> </div>
<div class="line">def hello_callback(detected_words):</div>
<div class="line">    print(&quot;Detected: {}&quot;.format(detected_words))</div>
<div class="line"> </div>
<div class="line">client = HowlClient()</div>
<div class="line">client.from_pretrained(&quot;hey_fire_fox&quot;, force_reload=False)</div>
<div class="line">client.add_listener(hello_callback)</div>
<div class="line">client.start().join()</div>
</div><!-- fragment --><h1>Training Guide</h1>
<h2>Installation</h2>
<ol type="1">
<li><code>git clone <a href="https://github.com/castorini/howl">https://github.com/castorini/howl</a> &amp;&amp; cd howl</code></li>
<li>Install <a href="https://pytorch.org">PyTorch</a> by following your platform-specific instructions.</li>
<li>Install PyAudio and its dependencies through your distribution's package system.</li>
<li><code>pip install -r requirements.txt -r requirements_training.txt</code> (some apt packages might need to be installed)</li>
<li><code>./download_mfa.sh</code> to setup montreal forced alginer (MFA) for dataset generation</li>
</ol>
<h2>Preparing a Dataset</h2>
<p>Assuming MFA is installed using <code>download_mfa.sh</code> and <a href="https://commonvoice.mozilla.org/">Common Voice dataset</a> is downloaded already, one can easily generate a dataset for custom wakeword using <code>generate_dataset.sh</code> script. </p><div class="fragment"><div class="line">./generate_dataset.sh &lt;common voice dataset path&gt; &lt;underscore separated wakeword (e.g. hey_fire_fox)&gt; &lt;inference sequence (e.g. [0,1,2])&gt; &lt;(Optional) &quot;true&quot; to skip negative dataset generation&gt;</div>
</div><!-- fragment --><p>In the example that follows, we describe the process of generating a dataste for the word, "fire."</p>
<ol type="1">
<li>Download a supported data source. We recommend <a href="https://commonvoice.mozilla.org/">Common Voice</a> for its breadth and free license.</li>
<li>To provide alignment for the data, install <a href="https://montreal-forced-aligner.readthedocs.io/en/stable/installation.html">Montreal Forced Aligner</a> (MFA) and download an <a href="http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b">English pronunciation dictionary</a>.</li>
<li>Create a positive dataset containing the keyword: <div class="fragment"><div class="line">VOCAB=&#39;[&quot;fire&quot;]&#39; INFERENCE_SEQUENCE=[0] DATASET_PATH=data/fire-positive python -m training.run.create_raw_dataset -i ~/path/to/common-voice --positive-pct 100 --negative-pct 0</div>
</div><!-- fragment --></li>
<li>Create a negative dataset without the keyword: note that 5% is sufficient when generating negative dataset from common-voice dataset <div class="fragment"><div class="line">VOCAB=&#39;[&quot;fire&quot;]&#39; INFERENCE_SEQUENCE=[0] DATASET_PATH=data/fire-negative python -m training.run.create_raw_dataset -i ~/path/to/common-voice --positive-pct 0 --negative-pct 5 </div>
</div><!-- fragment --></li>
<li>Generate some mock alignment for the negative set, where we don't care about alignment:</li>
</ol>
<div class="fragment"><div class="line">DATASET_PATH=data/fire-negative python -m training.run.attach_alignment --align-type stub</div>
</div><!-- fragment --><ol type="1">
<li>Use MFA to generate alignment for the positive set:</li>
</ol>
<div class="fragment"><div class="line">mfa_align data/fire-positive/audio eng.dict pretrained_models/english.zip output-folder</div>
</div><!-- fragment --><ol type="1">
<li>Attach the MFA alignment to the positive dataset:</li>
</ol>
<div class="fragment"><div class="line">DATASET_PATH=data/fire-positive python -m training.run.attach_alignment --align-type mfa -i output-folder</div>
</div><!-- fragment --><ol type="1">
<li>(Optional) Stitch vocab samples of aligned dataset to generate wakeword samples</li>
</ol>
<div class="fragment"><div class="line">VOCAB=&#39;[&quot;fire&quot;]&#39; INFERENCE_SEQUENCE=[0] python -m training.run.stitch_vocab_samples --aligned-dataset &quot;data/fire-positive&quot; --stitched-dataset &quot;data/fire-stitched&quot;</div>
</div><!-- fragment --><h2>Training and Running a Model</h2>
<ol type="1">
<li>Source the relevant environment variables for training the <code>res8</code> model: <code>source envs/res8.env</code>.</li>
<li>Train the model: <code>python -m <a class="el" href="namespacetraining_1_1run_1_1train.html">training.run.train</a> -i data/fire-positive data/fire-negative data/fire-stitched --model res8 --workspace workspaces/fire-res8</code>.</li>
<li>For the CLI demo, run <code>python -m <a class="el" href="namespacetraining_1_1run_1_1demo.html">training.run.demo</a> --model res8 --workspace workspaces/fire-res8</code>.</li>
</ol>
<p><code>train_model.sh</code> is also available which encaspulates individual command into a single bash script</p>
<div class="fragment"><div class="line">./train_model.sh &lt;env file path (e.g. envs/res8.env)&gt; &lt;model type (e.g. res8)&gt; &lt;workspace path (e.g. workspaces/fire-res8)&gt; &lt;dataset1 (e.g. data/fire-positive)&gt; &lt;dataset2(e.g. data/fire-negative)&gt; ...</div>
</div><!-- fragment --><h2>Pretrained Models</h2>
<p><a href="https://github.com/castorini/howl-models">howl-models</a> contains workspaces with pretrained models</p>
<p>To get the latest models, simply run <code>git submodule update --init --recursive</code></p>
<ul>
<li><a href="https://github.com/castorini/howl-models/tree/master/howl/hey-fire-fox">hey firefox</a></li>
</ul>
<div class="fragment"><div class="line">VOCAB=&#39;[&quot;hey&quot;,&quot;fire&quot;,&quot;fox&quot;]&#39; INFERENCE_SEQUENCE=[0,1,2] INFERENCE_THRESHOLD=0 NUM_MELS=40 MAX_WINDOW_SIZE_SECONDS=0.5 python -m training.run.demo --model res8 --workspace howl-models/howl/hey-fire-fox</div>
</div><!-- fragment --><h1>Reproducing Paper Results</h1>
<p>First, follow the installation instructions in the quickstart guide.</p>
<h2>Google Speech Commands</h2>
<ol type="1">
<li>Download <a href="https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html">the Google Speech Commands dataset</a> and extract it.</li>
<li>Source the appropriate environment variables: <code>source envs/res8.env</code></li>
<li>Set the dataset path to the root folder of the Speech Commands dataset: <code>export DATASET_PATH=/path/to/dataset</code></li>
<li>Train the <code>res8</code> model: &lsquo;NUM_EPOCHS=20 MAX_WINDOW_SIZE_SECONDS=1 VOCAB=&rsquo;["yes","no","up","down","left","right","on","off","stop","go"]' BATCH_SIZE=64 LR_DECAY=0.8 LEARNING_RATE=0.01 python -m <a class="el" href="namespacetraining_1_1run_1_1pretrain__gsc.html">training.run.pretrain_gsc</a> &ndash;model res8`</li>
</ol>
<h2>Hey Firefox</h2>
<ol type="1">
<li>Download <a href="https://nlp.nyc3.digitaloceanspaces.com/hey-ff-data.zip">the Hey Firefox corpus</a>, licensed under CC0, and extract it.</li>
<li>Download <a href="https://nlp.nyc3.digitaloceanspaces.com/hey-ff-noise.zip">our noise dataset</a>, built from Microsoft SNSD and MUSAN, and extract it.</li>
<li>Source the appropriate environment variables: <code>source envs/res8.env</code></li>
<li>Set the noise dataset path to the root folder: <code>export NOISE_DATASET_PATH=/path/to/snsd</code></li>
<li>Set the firefox dataset path to the root folder: <code>export DATASET_PATH=/path/to/hey_firefox</code></li>
<li>Train the model: &lsquo;LR_DECAY=0.98 VOCAB=&rsquo;["hey","fire","fox"]' USE_NOISE_DATASET=True BATCH_SIZE=16 INFERENCE_THRESHOLD=0 NUM_EPOCHS=300 NUM_MELS=40 INFERENCE_SEQUENCE=[0,1,2] MAX_WINDOW_SIZE_SECONDS=0.5 python -m <a class="el" href="namespacetraining_1_1run_1_1train.html">training.run.train</a> &ndash;model res8 &ndash;workspace workspaces/hey-ff-res8`</li>
</ol>
<h2>Hey Snips</h2>
<ol type="1">
<li>Download <a href="https://github.com/sonos/keyword-spotting-research-datasets">hey snips dataset</a></li>
<li>Process the dataset to a format howl can load</li>
</ol>
<div class="fragment"><div class="line">VOCAB=&#39;[&quot;hey&quot;,&quot;snips&quot;]&#39; INFERENCE_SEQUENCE=[0,1] DATASET_PATH=data/hey-snips python -m training.run.create_raw_dataset --dataset-type &#39;hey-snips&#39; -i ~/path/to/hey_snips_dataset</div>
</div><!-- fragment --><ol type="1">
<li>Generate some mock alignment for the dataset, where we don't care about alignment:</li>
</ol>
<div class="fragment"><div class="line">DATASET_PATH=data/hey-snips python -m training.run.attach_alignment --align-type stub</div>
</div><!-- fragment --><ol type="1">
<li>Use MFA to generate alignment for the dataset set:</li>
</ol>
<div class="fragment"><div class="line">mfa_align data/hey-snips/audio eng.dict pretrained_models/english.zip output-folder</div>
</div><!-- fragment --><ol type="1">
<li>Attach the MFA alignment to the dataset:</li>
</ol>
<div class="fragment"><div class="line">DATASET_PATH=data/hey-snips python -m training.run.attach_alignment --align-type mfa -i output-folder</div>
</div><!-- fragment --><ol type="1">
<li>Source the appropriate environment variables: <code>source envs/res8.env</code></li>
<li>Set the noise dataset path to the root folder: <code>export NOISE_DATASET_PATH=/path/to/snsd</code></li>
<li>Set the noise dataset path to the root folder: <code>export DATASET_PATH=/path/to/hey-snips</code></li>
<li>Train the model: &lsquo;LR_DECAY=0.98 VOCAB=&rsquo;["hey","snips"]' USE_NOISE_DATASET=True BATCH_SIZE=16 INFERENCE_THRESHOLD=0 NUM_EPOCHS=300 NUM_MELS=40 INFERENCE_SEQUENCE=[0,1] MAX_WINDOW_SIZE_SECONDS=0.5 python -m <a class="el" href="namespacetraining_1_1run_1_1train.html">training.run.train</a> &ndash;model res8 &ndash;workspace workspaces/hey-snips-res8`</li>
</ol>
<h2>Generating dataset for Mycroft-precise</h2>
<p>howl also provides a script for transforming howl dataset to <a href="https://github.com/MycroftAI/mycroft-precise">mycroft-precise</a> dataset </p><div class="fragment"><div class="line">VOCAB=&#39;[&quot;hey&quot;,&quot;fire&quot;,&quot;fox&quot;]&#39; INFERENCE_SEQUENCE=[0,1,2] python -m training.run.generate_precise_dataset --dataset-path /path/to/howl_dataset</div>
</div><!-- fragment --><h1>Experiments</h1>
<p>To verify the correctness of our implementation, we first train and evaluate our models on the Google Speech Commands dataset, for which there exists many known results. Next, we curate a wake word detection datasets and report our resulting model quality.</p>
<p>For both experiments, we generate reports in excel format. <a href="https://github.com/castorini/howl/tree/master/experiments">experiments</a> folder includes sample outputs from the for each experiment and corresponding workspaces can be found <a href="https://github.com/castorini/howl-models/tree/master/howl/experiments">here</a></p>
<h2>commands_recognition</h2>
<p>For command recognition, we train the four different models (res8, LSTM, LAS encoder, MobileNetv2) to detect twelve different keywords: “yes”, “no”, “up”, “down”, “left”, “right”, “on”, “off”, “stop”, “go”, unknown, or silence.</p>
<div class="fragment"><div class="line">python -m training.run.eval_commands_recognition --num_iterations n --dataset_path &lt; path_to_gsc_datasets &gt;</div>
</div><!-- fragment --><h2>word_detection</h2>
<p>In this experiment, we train our best commands recognition model, res8, for <code>hey firefox</code> and <code>hey snips</code> and evaluate them with different threashold.</p>
<p>Two different performance reports are generated, one with the clean audio and one with audios with noise</p>
<div class="fragment"><div class="line">python -m training.run.eval_wake_word_detection --num_models n --hop_size &lt; number between 0 and 1 &gt; --exp_type &lt; hey_firefox | hey_snips &gt; --dataset_path &quot;x&quot; --noiseset_path &quot;y&quot;</div>
</div><!-- fragment --><p>We also provide a script for generating ROC curve. <code>exp_timestamp</code> can be found from the reports generated from previous command</p>
<div class="fragment"><div class="line">python -m training.run.generate_roc --exp_timestamp &lt; experiment timestamp &gt; --exp_type &lt; hey_firefox | hey_snips &gt;</div>
</div><!-- fragment --> </div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
